{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwb425/class-2025-spring/blob/main/class-2025-spring_0328-0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers gradio accelerate\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Model identifier\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "\n",
        "# Load tokenizer and model with 8-bit quantization to reduce memory usage\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,  # Use 8-bit quantization\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create text generation pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define chat function\n",
        "def chat(prompt):\n",
        "    # Format the prompt using the chat template\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    response = pipe(formatted_prompt, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "    return response[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "# Launch Gradio interface\n",
        "gr.Interface(fn=chat, inputs=\"text\", outputs=\"text\", title=\"Zephyr-7B-Î± Chatbot\").launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfiiqiOfySf1lsGImM9sJF",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
