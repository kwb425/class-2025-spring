{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pEHOFr8vcKa"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwb425/class-2025-spring/blob/main/class-2025-spring_0530-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSW6Wx_ZjAQZ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccbA0CejRsV-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install datasets huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po4mMJ6chEmJ"
      },
      "source": [
        "### Data\n",
        "\n",
        "\n",
        "raw\n",
        "\tIn the latest AP dispatch from Frankfurt, which details how the European Union's chief trade negotiator said he had “good calls” with Trump administration officials on Monday, David McHugh noted that the EU \"has offered Trump a 'zero for zero' deal in which tariffs would be removed on industrial goods including automobiles, but the U.S. administration has said it will not lower tariffs below a 10% baseline imposed on almost all its trading partners. Trump has also announced tariffs of 25% on steel and automobiles.\"\n",
        "\tThat raises the question of where the negotiations, which got back on the rails over the weekend after a call between President Trump and EU Commission President Ursula von der Leyen, will go from here — and what Trump will have to say next time he discusses the $1.8 trillion trade relationship.\n",
        "\t...\n",
        "pretrain\n",
        "\tone record:\n",
        "\t\tIn the latest AP dispatch from Frankfurt, which details how the European Union's chief trade negotiator said he had “good calls” with Trump administration officials on Monday, David McHugh noted that the EU \"has offered Trump a 'zero for zero' deal in which tariffs would be removed on industrial goods including automobiles, but the U.S. administration has said it will not lower tariffs below a 10% baseline imposed on almost all its trading partners. Trump has also announced tariffs of 25% on steel and automobiles.\"\n",
        "\t\tThat raises the question of where the negotiations, which got back on the rails over the weekend after a call between President Trump and EU Commission President Ursula von der Leyen, will go from here — and what Trump will have to say next time he discusses the $1.8 trillion trade relationship.\n",
        "\t\t...\n",
        "\n",
        "\n",
        "\n",
        "instructional - alpaca\n",
        "\t[\n",
        "\t\t{\n",
        "\t\t\t\"Instruction\": \"Task we want the model to perform. 11111\",\n",
        "\t\t\t\"Input\": \"Optional, but useful, it will essentially be the user's query. 11111\",\n",
        "\t\t\t\"Output\": \"The expected result of the task and the output of the model 11111.\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"Instruction\": \"Task we want the model to perform. 22222\",\n",
        "\t\t\t\"Input\": \"Optional, but useful, it will essentially be the user's query. 22222\",\n",
        "\t\t\t\"Output\": \"The expected result of the task and the output of the model. 22222\"\n",
        "\t\t},\n",
        "\t]\n",
        "full fine-tuning | Lora fine-tuning\n",
        "\tone record:\n",
        "\t\t\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\t\tWrite a response that appropriately completes the request.\n",
        "\n",
        "\t\t### Instruction:\n",
        "\t\tTask we want the model to perform. 11111\n",
        "\n",
        "\t\t### Input:\n",
        "\t\tOptional, but useful, it will essentially be the user's query. 11111\n",
        "\n",
        "\t\t### Response:\n",
        "\t\tThe expected result of the task and the output of the model. 11111\"\"\"\n",
        "\tone record:\n",
        "\t\t\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\t\tWrite a response that appropriately completes the request.\n",
        "\n",
        "\t\t### Instruction:\n",
        "\t\tTask we want the model to perform. 22222\n",
        "\n",
        "\t\t### Input:\n",
        "\t\tOptional, but useful, it will essentially be the user's query. 22222\n",
        "\n",
        "\t\t### Response:\n",
        "\t\tThe expected result of the task and the output of the model. 22222\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "conversational - shareGPT\n",
        "{\n",
        "\t\"conversations\": [\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"system\",\n",
        "\t\t\t\"value\": \"You are a helpful assistant.\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"human\",\n",
        "\t\t\t\"value\": \"Can you help me make pasta carbonara?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"gpt\",\n",
        "\t\t\t\"value\": \"Would you like the traditional Roman recipe, or a simpler version?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"human\",\n",
        "\t\t\t\"value\": \"The traditional version please\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"gpt\",\n",
        "\t\t\t\"value\": \"The authentic Roman carbonara uses just a few ingredients: pasta, guanciale, eggs, Pecorino Romano, and black pepper. Would you like the detailed recipe?\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "full fine-tuning | Lora fine-tuning\n",
        "\tone record:\n",
        "\t\t<|im_start|>system\n",
        "\t\tYou are a helpful assistant.<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tCan you help me make pasta carbonara?<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tWould you like the traditional Roman recipe, or a simpler version?<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tThe traditional version please<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tThe authentic Roman carbonara uses just a few ingredients: pasta, guanciale, eggs, Pecorino Romano, and black pepper. Would you like the detailed recipe?<|im_end|>\n",
        "\n",
        "\n",
        "\n",
        "conversational - ChatML\n",
        "{\n",
        "\t\"messages\": [\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"system\",\n",
        "\t\t\t\"content\": \"You are a helpful assistant.\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"user\",\n",
        "\t\t\t\"content\": \"What is 1+1?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"assistant\",\n",
        "\t\t\t\"content\": \"It's 2!\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"user\",\n",
        "\t\t\t\"content\": \"What is 2+2?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"assistant\",\n",
        "\t\t\t\"content\": \"It's 4!\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "full fine-tuning | Lora fine-tuning\n",
        "\tone record:\n",
        "\t\t<|im_start|>system\n",
        "\t\tYou are a helpful assistant.<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tWhat is 1+1?<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tIt's 2!<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tWhat is 2+2?<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tIt's 4!<|im_end|>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCFFT_O4g-dB"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub import create_repo, delete_repo\n",
        "import json\n",
        "\n",
        "\n",
        "# login(token=\"key\")\n",
        "\n",
        "with open('class-2025-spring_0530-0.json', 'r') as f:\n",
        "    loaded_data = json.load(f)\n",
        "\n",
        "non_reasoning_dataset = Dataset.from_list(loaded_data)\n",
        "# delete_repo(repo_id=\"userID/2025-spring-conversations\", repo_type=\"dataset\")\n",
        "# create_repo(repo_id=\"userID/2025-spring-conversations\", repo_type=\"dataset\", private=False, exist_ok=True)\n",
        "# non_reasoning_dataset.push_to_hub(\"userID/2025-spring-conversations\")\n",
        "# from datasets import load_dataset\n",
        "# non_reasoning_dataset = load_dataset(\"userID/2025-spring-conversations\", split = \"train\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# https://huggingface.co/unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "# LoRA adapters, updating 1 to 10% of all parameters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # rank, 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # Rank stabilized LoRA\n",
        "    loftq_config = None,  # LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zoaygOAe3I2"
      },
      "outputs": [],
      "source": [
        "non_reasoning_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBFaeQHfSxp"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
        "\n",
        "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    dataset[\"conversations\"],\n",
        "    tokenize = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0hbEekfeqf"
      },
      "outputs": [],
      "source": [
        "non_reasoning_conversations[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-e0KO9GgFy3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "non_reasoning_subset = pd.Series(non_reasoning_conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfV47_SXgXH4"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([\n",
        "    pd.Series(non_reasoning_subset)\n",
        "])\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "non_reasoning_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "non_reasoning_dataset = non_reasoning_dataset.shuffle(seed = 3407)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = non_reasoning_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"자유심증주의란 무엇인가요? 어떻게 적용되나요? 헷갈리는데?\"},\n",
        "    # {\"role\" : \"user\", \"content\" : \"경복궁이 뭐에요?\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j873RMcEi9uq"
      },
      "outputs": [],
      "source": [
        "# messages = [\n",
        "#     {\"role\" : \"user\", \"content\" : \"자백의 보강법칙이란? 무엇인가요? 알려주세요!\"}\n",
        "# ]\n",
        "# text = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize = False,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     enable_thinking = True, # Disable thinking\n",
        "# )\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# _ = model.generate(\n",
        "#     **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "#     max_new_tokens = 1024, # Increase for longer outputs!\n",
        "#     temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
        "#     streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained(\"my_lora_model\")  # Local saving\n",
        "# tokenizer.save_pretrained(\"my_lora_model\")\n",
        "model.push_to_hub(\"userId/my_lora_model\", token = \"key\") # Online saving\n",
        "tokenizer.push_to_hub(\"userId/my_lora_model\", token = \"key\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Loading trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"userID/my_lora_model\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "Select `merged_16bit` for float16 or `merged_4bit` for int4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"key\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"key\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}